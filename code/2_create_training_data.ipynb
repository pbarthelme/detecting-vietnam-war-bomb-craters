{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training data\n",
    "This notebook takes the selected image tiles (geotiff files) and the corresponding labels (geojson files with crater polygons manually created in QGIS) and transforms them into the training data for the neural network. The geojson files are rasterized and an additional boundary class for instance segmentation is added. The final data, consisting of matching image pairs (256 $\\times$ 256 pixels) of KH-9 image and label, are split into training, validation and test set and saved in an h5py file.\n",
    "\n",
    "#### Inputs:\n",
    "* *tile_catalog_path*: Tile catalog with metadata and extent of each of the selected image tiles (geojson file)\n",
    "* *labels_processed_path*: File with labelled craters for each study area (geojson files)\n",
    "\n",
    "#### Parameters:\n",
    "* *study_areas*: Names of the study_areas\n",
    "* *crater_ids*: Integers that represent craters in the labelled image tiles\n",
    "* *boundary_id*: Integer that represents the boundary class in the labelled image tiles\n",
    "* *n_classes*: Number of classes in the labelled image tiles = len(crater_ids) + 2 (boundary and background classes)\n",
    "* *min_crater_area*: Minimum area (in pixels) of a crater in the labelled image tiles (smaller craters are removed)\n",
    "* *study_areas -> [study_area] -> train_prop*: Proportion of image tiles to use for the training data\n",
    "* *study_areas -> [study_area] -> val_prop*: Proportion of image tiles to use for the validation data\n",
    "* *study_areas -> [study_area] -> test_prop*: Proportion of image tiles to use for the test data\n",
    "\n",
    "#### Outputs:\n",
    "* *tile_labels_folder*: Created label tiles for each selected KH-9 image tiles (256 $\\times$ 256 pixels) (geotiff files)\n",
    "* *tile_catalog_labels_path*: Same as *tile_catalog_path* but includes path to the labelled image tile in *tile_labels_folder*\n",
    "* *data_path*: Matching image pairs (256 $\\times$ 256 pixels) of KH-9 image and crater label split into training, validation and test sets (hdf5 file)\n",
    "* *data_path_sa*: Same as *data_path* but split into the separate study areas (hdf5 files)\n",
    "\n",
    "#### Created paper content:\n",
    "* **Crater prevalence**: Prevalence of crater pixels in the labelled data for each study area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import h5py\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import rasterio.features\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage import morphology, segmentation\n",
    "from utils import create_dir, load_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boundaries(crater_id, boundary_distance=2):\n",
    "    crater_id_expanded = segmentation.expand_labels(\n",
    "        crater_id, distance=boundary_distance)\n",
    "\n",
    "    # get all pixels where the expanded borders are now touching and define those as border class\n",
    "    border = segmentation.find_boundaries(\n",
    "        crater_id_expanded, mode=\"outer\", connectivity=2, background=0)\n",
    "    border[crater_id_expanded == 0] = 0\n",
    "    # fill small holes in the border that can occur if boundaries are touching diagonally\n",
    "    # (due to connectivity=2 setting in find_boundaries function above)\n",
    "    border = morphology.remove_small_holes(\n",
    "        border, area_threshold=2, connectivity=1)\n",
    "\n",
    "    # get all boundary pixels that are not border pixels\n",
    "    boundaries = crater_id_expanded != crater_id\n",
    "    boundaries[border] = 0\n",
    "\n",
    "    return boundaries > 0, border > 0\n",
    "\n",
    "\n",
    "def write_label_tiles(catalog, labels_path, out_dir, boundary_id, min_crater_area=25):\n",
    "    '''\n",
    "    Writes out individual tiles from a raster file and updates the tile catalog\n",
    "    with the corresponding output path\n",
    "    '''\n",
    "    labels = gpd.read_file(labels_path)\n",
    "\n",
    "    create_dir(out_dir)\n",
    "\n",
    "    shapes_class = [(geom, val)\n",
    "                    for geom, val in zip(labels.geometry, labels.class_id)]\n",
    "    shapes_id = [(geom, val) for geom, val in zip(labels.geometry, labels.id)]\n",
    "\n",
    "    for idx, row in catalog.iterrows():\n",
    "        with rasterio.open(row[\"tile\"]) as src:\n",
    "\n",
    "            img_labels = rasterio.features.rasterize(\n",
    "                shapes_class,\n",
    "                fill=0,\n",
    "                out_shape=src.shape,\n",
    "                transform=src.transform,\n",
    "                dtype=\"uint8\"\n",
    "            )\n",
    "            crater_id = rasterio.features.rasterize(\n",
    "                shapes_id,\n",
    "                fill=0,\n",
    "                out_shape=src.shape,\n",
    "                transform=src.transform,\n",
    "                dtype=\"uint32\"\n",
    "            )\n",
    "\n",
    "            # add boundary and border pixels to the labels\n",
    "            boundary_mask, border_mask = get_boundaries(crater_id)\n",
    "\n",
    "            # convert craters smaller than min_crater_area to background class, do this after boundary has been\n",
    "            # calculated as boundary class can decrease crater size for overlapping craters\n",
    "            # this is done to ensure small objects removals are done in a consistent way on both labels and predictions\n",
    "            crater_id[boundary_mask] = 0\n",
    "            crater_id[border_mask] = 0\n",
    "\n",
    "            large_objects_mask = morphology.remove_small_objects(\n",
    "                crater_id > 0, min_size=min_crater_area)\n",
    "            crater_id[~large_objects_mask] = 0\n",
    "            img_labels[~large_objects_mask] = 0\n",
    "\n",
    "            # recalculate the boundary pixels of remaining craters\n",
    "            # (otherwise boundaries of removed craters are still present)\n",
    "            boundary_mask, border_mask = get_boundaries(crater_id)\n",
    "\n",
    "            img_labels[boundary_mask] = boundary_id\n",
    "            img_labels[border_mask] = boundary_id\n",
    "\n",
    "            profile = src.profile\n",
    "            profile.update(dtype=\"uint8\", nodata=255)\n",
    "\n",
    "            fn = row[\"fn\"]\n",
    "            out_path = f\"{out_dir}/{fn}\"\n",
    "            with rasterio.open(out_path, \"w\", **profile) as dst:\n",
    "                dst.write(img_labels, 1)\n",
    "            catalog.at[idx, \"tile_label\"] = out_path\n",
    "\n",
    "    return catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_into_sets(df, train_prop, val_prop, test_prop, random_state=42):\n",
    "    \"\"\"\n",
    "    Split the DataFrame into train, validation, and test sets.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        train_prop (float): Proportion of data to be used for training.\n",
    "        val_prop (float): Proportion of data to be used for validation.\n",
    "        test_prop (float): Proportion of data to be used for testing.\n",
    "        random_state (int): Random seed for reproducibility (default is 42).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The input DataFrame with three new columns \"train\", \"val\", and \"test\".\n",
    "    \"\"\"\n",
    "\n",
    "    # Check if the proportions add up to 1\n",
    "    total_proportions = train_prop + val_prop + test_prop\n",
    "    if abs(total_proportions - 1.0) > 1e-6:\n",
    "        raise ValueError(\"Proportions should add up to 1.0\")\n",
    "\n",
    "    # Step 1: Split the data into train-test and validation sets\n",
    "    train_data, test_val_data = train_test_split(\n",
    "        df, train_size=train_prop, random_state=random_state)\n",
    "\n",
    "    # Step 2: Split the train-test data into train and test sets\n",
    "    val_data, test_data = train_test_split(\n",
    "        test_val_data, test_size=test_prop / (val_prop + test_prop), random_state=random_state)\n",
    "\n",
    "    # Step 3: Create new columns in the DataFrame to indicate the split\n",
    "    df[\"train\"] = 0\n",
    "    df[\"val\"] = 0\n",
    "    df[\"test\"] = 0\n",
    "\n",
    "    # Assign 1 to the rows that belong to the corresponding sets\n",
    "    df.loc[train_data.index, \"train\"] = 1\n",
    "    df.loc[val_data.index, \"val\"] = 1\n",
    "    df.loc[test_data.index, \"test\"] = 1\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_training_data(catalog, out_file, n_classes, tile_col=\"tile\", label_col=\"tile_label\"):\n",
    "    \"\"\"\n",
    "    Create training data in HDF5 format from the catalog DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        catalog (pd.DataFrame): The catalog DataFrame containing file paths and subset information.\n",
    "        out_file (str): The output HDF5 file path to store the training data.\n",
    "        n_classes (int): The number of classes for one-hot encoding.\n",
    "        tile_col (str): The column name in the catalog DataFrame containing tile file paths (default is \"tile\").\n",
    "        label_col (str): The column name in the catalog DataFrame containing label file paths (default is \"tile_label\").\n",
    "\n",
    "    Notes:\n",
    "        The catalog DataFrame should contain columns for \"train\", \"val\", and \"test\" subsets, where each value is 0 or 1\n",
    "        indicating if the sample belongs to that subset.\n",
    "\n",
    "        The output HDF5 file will have datasets named \"x_train\", \"x_val\", and \"x_test\" containing input images,\n",
    "        and \"y_train\", \"y_val\", and \"y_test\" containing one-hot encoded label arrays.\n",
    "    \"\"\"\n",
    "    res = {}\n",
    "    subsets = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "    for subset in subsets:\n",
    "        catalog_subset = catalog[catalog[subset] == 1]\n",
    "\n",
    "        imgs = list()\n",
    "        labels = list()\n",
    "        for idx, row in catalog_subset.iterrows():\n",
    "            with rasterio.open(row[tile_col]) as src:\n",
    "                imgs.append(src.read(1))\n",
    "            with rasterio.open(row[label_col]) as src:\n",
    "                label = src.read(1)\n",
    "\n",
    "            class_labels = list()\n",
    "            for i in range(n_classes):\n",
    "                class_labels.append(label == i)\n",
    "            labels.append(np.stack(class_labels, axis=-1))\n",
    "\n",
    "        imgs = np.stack(imgs)\n",
    "        imgs = np.expand_dims(imgs, axis=-1)\n",
    "        labels = np.stack(labels).astype(\"float32\")\n",
    "\n",
    "        assert (labels.sum(axis=-1) == 1).all(), (\n",
    "            \"Each pixel needs to have exactly one class assigned to it\")\n",
    "\n",
    "        res[f\"x_{subset}\"] = imgs\n",
    "        res[f\"y_{subset}\"] = labels\n",
    "\n",
    "    create_dir(out_file, is_file=True)\n",
    "    with h5py.File(out_file, \"w\") as hf:\n",
    "        for k, v in res.items():\n",
    "            hf.create_dataset(k, data=v, compression=\"gzip\")\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_subset(path, subset):\n",
    "    \"\"\"\n",
    "    Read a specific subset of data from an HDF5 file.\n",
    "\n",
    "    Parameters:\n",
    "        path (str): The file path to the HDF5 file.\n",
    "        subset (str): The name of the subset to read.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: The data from the specified subset.\n",
    "    \"\"\"\n",
    "    with h5py.File(path, \"r\") as hf:\n",
    "        return hf[subset][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(\"../config.yaml\")\n",
    "study_areas = config.get(\"study_areas\").keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory created: ../data/2_training_data/quang_tri/tile_labels\n",
      "Directory already exists: ../data/2_training_data/quang_tri\n",
      "Directory created: ../data/2_training_data/tri_border_area/tile_labels\n",
      "Directory already exists: ../data/2_training_data/tri_border_area\n"
     ]
    }
   ],
   "source": [
    "for study_area in study_areas:\n",
    "    catalog = gpd.read_file(\n",
    "        config[\"tile_catalog_path\"].format(study_area=study_area)\n",
    "    )\n",
    "\n",
    "    catalog = write_label_tiles(\n",
    "        catalog=catalog,\n",
    "        labels_path=config[\"labels_processed_path\"].format(\n",
    "            study_area=study_area),\n",
    "        out_dir=config[\"tile_labels_folder\"].format(study_area=study_area),\n",
    "        boundary_id=config[\"boundary_id\"],\n",
    "        min_crater_area=config[\"min_crater_area\"]\n",
    "    )\n",
    "\n",
    "    catalog = split_data_into_sets(\n",
    "        catalog,\n",
    "        train_prop=config.get(\"study_areas\").get(study_area).get(\"train_prop\"),\n",
    "        val_prop=config.get(\"study_areas\").get(study_area).get(\"val_prop\"),\n",
    "        test_prop=config.get(\"study_areas\").get(study_area).get(\"test_prop\")\n",
    "    )\n",
    "\n",
    "    catalog.to_file(config[\"tile_catalog_labels_path\"].format(\n",
    "        study_area=study_area), driver=\"GeoJSON\")\n",
    "\n",
    "    create_training_data(\n",
    "        catalog,\n",
    "        out_file=config.get(\"data_path_sa\").format(study_area=study_area),\n",
    "        n_classes=config.get(\"n_classes\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the study areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_study_areas_data(study_areas, data_path_sa, data_path):\n",
    "    \"\"\"\n",
    "    Combine data from all study areas and store it in a single HDF5 file.\n",
    "\n",
    "    Parameters:\n",
    "        study_areas (list): A list of the names of all study areas to be combined.\n",
    "        data_path_sa (str): A string template for the data paths with \"{study_area}\" as a placeholder.\n",
    "        data_path (str): The output file path to store the combined data.\n",
    "\n",
    "    Notes:\n",
    "        The study_areas dictionary should have study area names as keys and corresponding data paths as values.\n",
    "    \"\"\"\n",
    "    data_paths = [data_path_sa.format(study_area=i) for i in study_areas]\n",
    "    subsets = [\"x_train\", \"x_val\", \"x_test\", \"y_train\", \"y_val\", \"y_test\"]\n",
    "\n",
    "    res = [[read_subset(path, subset) for path in data_paths] for subset in subsets]\n",
    "    res_type = [np.concatenate(i) for i in res]\n",
    "\n",
    "    with h5py.File(data_path, \"w\") as hf:\n",
    "        for k, v in zip(subsets, res_type):\n",
    "            hf.create_dataset(k, data=v, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_study_areas_data(\n",
    "    study_areas=config.get(\"study_areas\").keys(),\n",
    "    data_path_sa=config.get(\"data_path_sa\"),\n",
    "    data_path=config.get(\"data_path\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['x_test', 'x_train', 'x_val', 'y_test', 'y_train', 'y_val']>\n",
      "(1200, 256, 256, 1) (1200, 256, 256, 7)\n",
      "(400, 256, 256, 1) (400, 256, 256, 7)\n",
      "(800, 256, 256, 1) (800, 256, 256, 7)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(config.get(\"data_path\"), \"r\") as hf:\n",
    "    print(hf.keys())\n",
    "    x_train = hf[\"x_train\"][:]\n",
    "    y_train = hf[\"y_train\"][:]\n",
    "    x_val = hf[\"x_val\"][:]\n",
    "    y_val = hf[\"y_val\"][:]\n",
    "    x_test = hf[\"x_test\"][:]\n",
    "    y_test = hf[\"y_test\"][:]\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_val.shape, y_val.shape)\n",
    "print(x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate crater prevalence per study area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_prevalence(study_area, data_path_sa, crater_ids):\n",
    "    data_path = data_path_sa.format(study_area=study_area)\n",
    "    subsets = [\"y_train\", \"y_val\", \"y_test\"]\n",
    "\n",
    "    y = np.concatenate([read_subset(data_path, subset) for subset in subsets])\n",
    "    y_argmax = np.argmax(y, axis=-1)\n",
    "\n",
    "    n_crater_pixels = np.isin(y_argmax, crater_ids).sum()\n",
    "    n_pixels = y_argmax.size\n",
    "    prev = np.round(n_pixels / n_crater_pixels)\n",
    "\n",
    "    print(f\"Study Area: {study_area}\")\n",
    "    print(f\"{n_crater_pixels} crater pixels out of {n_pixels} total pixels.\")\n",
    "    print(f\"1 in {prev} pixels are crater pixels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Study Area: quang_tri\n",
      "724603 crater pixels out of 65536000 total pixels.\n",
      "1 in 90.0 pixels are crater pixels\n",
      "Study Area: tri_border_area\n",
      "63970 crater pixels out of 91750400 total pixels.\n",
      "1 in 1434.0 pixels are crater pixels\n"
     ]
    }
   ],
   "source": [
    "for study_area in study_areas:\n",
    "    calc_prevalence(study_area, config.get(\"data_path_sa\"),\n",
    "                    crater_ids=config.get(\"crater_ids\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
